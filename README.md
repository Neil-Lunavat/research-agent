# Research Agent Phase 1 Scraper

A standalone Python scraper that collects the latest tech news, research papers, and discussions from multiple sources without requiring authentication.

## ğŸ¯ What it does

Scrapes data from:
- **Hacker News** - Latest tech discussions via Algolia API
- **ArXiv** - Research papers from computer science categories
- **RSS Feeds** - TechCrunch, MIT Tech Review, Wired

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### 2. Run the Scraper

```bash
python phase1_scraper.py
```

You'll be prompted to enter:
- **Topic**: What to search for (e.g., "AI", "blockchain", "startups")
- **Days**: How many days back to search (default: 7)

### 3. View Results

The scraper creates two files in the `data/` directory:
- **JSON file**: Raw structured data for further processing
- **Markdown file**: Human-readable report

## ğŸ“Š Example Output

```
ğŸ¤– Research Agent Phase 1 Scraper
========================================
Enter research topic (e.g., 'AI', 'blockchain', 'startups'): AI
Enter number of days to search (default: 7): 7

ğŸš€ Starting Phase 1 scraping for topic: 'AI' (last 7 days)
============================================================
ğŸ” Fetching Hacker News stories for 'AI' (last 7 days)...
âœ… Found 15 Hacker News articles
ğŸ“š Fetching ArXiv papers for 'AI' (last 7 days)...
  ğŸ“¡ Fetching from http://export.arxiv.org/rss/cs.AI
  ğŸ“¡ Fetching from http://export.arxiv.org/rss/cs.LG
âœ… Found 8 ArXiv papers
ğŸ“° Fetching RSS feeds for 'AI' (last 7 days)...
  ğŸ“¡ Fetching from techcrunch: https://techcrunch.com/feed/
  ğŸ“¡ Fetching from mit_tech_review: https://www.technologyreview.com/feed/
  ğŸ“¡ Fetching from wired: https://www.wired.com/feed/
âœ… Found 12 RSS articles

============================================================
ğŸ‰ Scraping complete! Total articles found: 35
   ğŸ“Š Hacker News: 15
   ğŸ“š ArXiv: 8
   ğŸ“° RSS Feeds: 12

ğŸ’¾ Results saved to: data/research_AI_20241210_143022.json
ğŸ“ Markdown report saved to: data/research_report_AI_20241210_143022.md
```

## ğŸ”§ Features

- **No Authentication Required**: Uses public APIs and RSS feeds
- **Smart Topic Mapping**: Automatically maps topics to relevant ArXiv categories
- **Date Filtering**: Only fetches recent content within specified timeframe
- **Duplicate Handling**: Basic filtering to avoid duplicate content
- **Multiple Output Formats**: Both JSON (for processing) and Markdown (for reading)
- **Error Handling**: Gracefully handles API failures and network issues
- **Respectful Scraping**: Includes delays and proper headers

## ğŸ“ Data Structure

### JSON Output
```json
{
  "hackernews": [...],
  "arxiv": [...],
  "rss": [...],
  "metadata": {
    "topic": "AI",
    "days": 7,
    "scraped_at": "2024-12-10T14:30:22",
    "total_articles": 35
  }
}
```

### Article Fields
- **Hacker News**: title, url, points, comments_count, author, created_at, hn_url
- **ArXiv**: title, authors, abstract, pdf_url, arxiv_url, published_date, category
- **RSS**: title, description, url, published_date, source, author

## ğŸ¨ Customization

You can easily modify the scraper by editing `phase1_scraper.py`:

- **Add RSS sources**: Update the `rss_sources` dictionary
- **Add ArXiv categories**: Update the `arxiv_categories` dictionary  
- **Change topic matching**: Modify the `_matches_topic()` method
- **Adjust timeouts**: Change the timeout values in API calls

## ğŸš§ Limitations

- **Simple topic matching**: Uses basic keyword matching (could be improved with NLP)
- **No content extraction**: Doesn't fetch full article text (can be added with newspaper3k)
- **Rate limiting**: Basic delays only (could be enhanced)
- **No deduplication**: Doesn't remove similar articles across sources

## ğŸ”® Next Steps

This is Phase 1 of a larger research agent. Future phases will add:
- Reddit API integration
- GitHub trending repositories
- Advanced web scraping for protected sites
- LLM-powered content analysis and idea generation
- Streamlit web interface

## ğŸ“ License

This project is for educational/research purposes. Please respect the terms of service of all data sources. 