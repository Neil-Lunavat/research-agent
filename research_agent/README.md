# Research Agent (CLI)

A headless research agent that scrapes, preprocesses, embeds, stores, validates market signals, and emails reports.

## ğŸš€ Features

- **Multi-source data collection**: Hacker News (Algolia), ArXiv RSS, RSS (TechCrunch, MIT Tech Review, Wired)
- **Content extraction**: Optional full-article crawl via crawl4ai
- **Storage**: SQLite (`data/research.db`) for metadata + content
- **Embeddings**: Local SentenceTransformers + Chroma (disk persistence)
- **Socials + Validation**: Lightweight Reddit search + heuristic market score
- **Reporting + Email**: Markdown report + SMTP email

## ğŸ“ Project Structure

```
research_agent/
â”œâ”€â”€ cli.py                      # CLI entrypoint
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ api_scrapers.py     # HN + ArXiv
â”‚   â”‚   â”œâ”€â”€ rss_scrapers.py     # TechCrunch, MIT TR, Wired
â”‚   â”‚   â”œâ”€â”€ base_scraper.py     # Orchestrator
â”‚   â”‚   â””â”€â”€ social_scrapers.py  # Reddit (lightweight)
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ content_processor.py
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â””â”€â”€ validator.py
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ file_manager.py
â”‚   â”‚   â””â”€â”€ db.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ emailer.py
â”‚   â””â”€â”€ vector_store/
â”‚       â””â”€â”€ vector_store.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ research.db             # SQLite DB (created at runtime)
â”‚   â””â”€â”€ reports/                # Generated reports
â”œâ”€â”€ vector_store/               # Chroma persistence
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ Installation

1. **Clone or create the project directory**:
   ```bash
   cd research_agent
   ```

2. **Create a virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Environment variables** (optional for email):

Create `.env` in `research_agent/`:

```
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your_email@gmail.com
SMTP_PASS=your_app_password
EMAIL_FROM=your_email@gmail.com
EMAIL_TO=recipient@example.com
```

## ğŸš€ Usage (CLI)

- Ingest scrape -> DB -> embeddings (optionally crawl content):

```
python cli.py ingest --topic "AI" --days 7 --include-content --save-json --save-md
```

- Validate market (social + DB metrics), write report, optionally email:

```
python cli.py validate --topic "AI" --days 7 --email
```

- Semantic search over embedded corpus:

```
python cli.py search --query "foundation models for robotics" -k 5
```

### Programmatic Usage

```python
import asyncio
from src.scrapers.base_scraper import ResearchAgent

# Initialize the agent
agent = ResearchAgent()

# Basic scraping (metadata only)
results = agent.scrape_all("AI", days=7)

# Enhanced scraping with content extraction
results = asyncio.run(agent.scrape_all_with_content("AI", days=7))

# Save results
agent.save_results(results)
agent.export_to_markdown(results)
```

## ğŸ“Š Data Sources

- **Hacker News**: Algolia API (no auth)
- **ArXiv**: RSS feeds for multiple CS categories
- **TechCrunch/MIT TR/Wired**: RSS feeds
- **Reddit (light)**: public JSON search (no auth)

## ğŸ”§ Configuration

Works out of the box without email. Data is saved to `research_agent/data/` and vectors to `research_agent/vector_store/`.

## ğŸ“ˆ Future Enhancements

- PRAW-based Reddit + GitHub Trending APIs
- LLM summarization/idea generation pipeline
- Better dedup and source config
- Scheduling via cron/systemd timers

## ğŸ¤ Contributing

This is a modular architecture designed for easy extension. To add new data sources:

1. Create a new scraper in `src/scrapers/`
2. Add the scraper to `base_scraper.py`
3. Update the UI to display the new source

## ğŸ“„ License

This project is for educational and research purposes.